{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d1a0edf-2c3f-484d-acaa-e405fb559960",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, contains, when, sum, avg, round,max, row_number, desc\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import IntegerType,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "45ac9077-10f1-41c7-b94d-087f5c136e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"ExtractNestedData\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7a8082a4-dff2-4d48-bdf1-ee652453acb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"/Users/benjaminbrooke/Desktop/Data/dicubed_sdtm_csv_2019-05-30/tr.csv\",header = True)\n",
    "df = df.drop(\"_c0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bf4f0d61-1ff4-4238-aa6b-b99cdffc6a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"id\",row_number().over(Window.partitionBy(\"STUDYID\").orderBy(\"TRDTC\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "face948e-87c7-4a23-b1b5-e4fb780abfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"STUDYID\", \"DOMAIN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5012ffde-ed14-47e0-b6e2-cd79d3e0fa2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-------+--------+----------------+-------+--------+--------+--------+--------+--------+--------+-----+----------+---+\n",
      "|   USUBJID|TRSEQ|TRLNKID|TRTESTCD|          TRTEST|TRORRES|TRORRESU|TRSTRESC|TRSTRESN|TRSTRESU|TRMETHOD|VISITNUM|VISIT|     TRDTC| id|\n",
      "+----------+-----+-------+--------+----------------+-------+--------+--------+--------+--------+--------+--------+-----+----------+---+\n",
      "|UCSF-BR-09|   41|    T01|  VOLUME|          Volume|  29.90|      mL|   29.90|    29.9|      mL|     MRI|       1|  MR1|1986-11-26|  1|\n",
      "|UCSF-BR-09|   42|    T01|   LDIAM|Longest Diameter|  64.00|      mm|   64.00|      64|      mm|     MRI|       1|  MR1|1986-11-26|  2|\n",
      "|UCSF-BR-09|   43|    T01|  VOLUME|          Volume|  20.00|      mL|   20.00|      20|      mL|     MRI|       3|  MR3|1987-01-27|  3|\n",
      "|UCSF-BR-09|   44|    T01|   LDIAM|Longest Diameter|  64.00|      mm|   64.00|      64|      mm|     MRI|       3|  MR3|1987-01-27|  4|\n",
      "|UCSF-BR-67|  369|    T01|   LDIAM|Longest Diameter|  61.70|      mm|   61.70|    61.7|      mm|     MRI|       1|  MR1|1988-04-21|  5|\n",
      "|UCSF-BR-67|  370|    T01|  VOLUME|          Volume|  30.10|      mL|   30.10|    30.1|      mL|     MRI|       1|  MR1|1988-04-21|  6|\n",
      "|UCSF-BR-13|   61|    T01|  VOLUME|          Volume|  66.70|      mL|   66.70|    66.7|      mL|     MRI|       1|  MR1|1988-04-25|  7|\n",
      "|UCSF-BR-13|   62|    T01|   LDIAM|Longest Diameter|  91.10|      mm|   91.10|    91.1|      mm|     MRI|       1|  MR1|1988-04-25|  8|\n",
      "|UCSF-BR-18|   81|    T01|  VOLUME|          Volume|  26.50|      mL|   26.50|    26.5|      mL|     MRI|       1|  MR1|1988-06-16|  9|\n",
      "|UCSF-BR-18|   82|    T01|   LDIAM|Longest Diameter|  71.00|      mm|   71.00|      71|      mm|     MRI|       1|  MR1|1988-06-16| 10|\n",
      "+----------+-----+-------+--------+----------------+-------+--------+--------+--------+--------+--------+--------+-----+----------+---+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0c5186a8-df89-4f14-81ec-39f6274130eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_cats = df[col(\"id\").isin(1,2,3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c06278c9-919a-4e1f-9469-753f889a813d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-------+--------+----------------+-------+--------+--------+--------+--------+--------+--------+-----+----------+---+\n",
      "|   USUBJID|TRSEQ|TRLNKID|TRTESTCD|          TRTEST|TRORRES|TRORRESU|TRSTRESC|TRSTRESN|TRSTRESU|TRMETHOD|VISITNUM|VISIT|     TRDTC| id|\n",
      "+----------+-----+-------+--------+----------------+-------+--------+--------+--------+--------+--------+--------+-----+----------+---+\n",
      "|UCSF-BR-09|   41|    T01|  VOLUME|          Volume|  29.90|      mL|   29.90|    29.9|      mL|     MRI|       1|  MR1|1986-11-26|  1|\n",
      "|UCSF-BR-09|   42|    T01|   LDIAM|Longest Diameter|  64.00|      mm|   64.00|      64|      mm|     MRI|       1|  MR1|1986-11-26|  2|\n",
      "|UCSF-BR-09|   43|    T01|  VOLUME|          Volume|  20.00|      mL|   20.00|      20|      mL|     MRI|       3|  MR3|1987-01-27|  3|\n",
      "|ISPY1_1001|    1|    T01|   LDIAM|Longest Diameter|     88|      mm|      88|      88|      mm|     MRI|       1|  MR1|1984-10-13|  1|\n",
      "|ISPY1_1002|    5|    T01|   LDIAM|Longest Diameter|     29|      mm|      29|      29|      mm|     MRI|       1|  MR1|1984-11-02|  2|\n",
      "|ISPY1_1001|    2|    T01|   LDIAM|Longest Diameter|     78|      mm|      78|      78|      mm|     MRI|       2|  MR2|1984-11-08|  3|\n",
      "+----------+-----+-------+--------+----------------+-------+--------+--------+--------+--------+--------+--------+-----+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check_cats.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e418f464-01c5-43a3-8f22-f4c552f88443",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_cats = df.filter(col(\"id\") <= 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0429b299-b0a8-4254-b216-9368483cd841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-------+--------+----------------+-------+--------+--------+--------+--------+--------+--------+-----+----------+---+\n",
      "|   USUBJID|TRSEQ|TRLNKID|TRTESTCD|          TRTEST|TRORRES|TRORRESU|TRSTRESC|TRSTRESN|TRSTRESU|TRMETHOD|VISITNUM|VISIT|     TRDTC| id|\n",
      "+----------+-----+-------+--------+----------------+-------+--------+--------+--------+--------+--------+--------+-----+----------+---+\n",
      "|UCSF-BR-09|   41|    T01|  VOLUME|          Volume|  29.90|      mL|   29.90|    29.9|      mL|     MRI|       1|  MR1|1986-11-26|  1|\n",
      "|UCSF-BR-09|   42|    T01|   LDIAM|Longest Diameter|  64.00|      mm|   64.00|      64|      mm|     MRI|       1|  MR1|1986-11-26|  2|\n",
      "|UCSF-BR-09|   43|    T01|  VOLUME|          Volume|  20.00|      mL|   20.00|      20|      mL|     MRI|       3|  MR3|1987-01-27|  3|\n",
      "|ISPY1_1001|    1|    T01|   LDIAM|Longest Diameter|     88|      mm|      88|      88|      mm|     MRI|       1|  MR1|1984-10-13|  1|\n",
      "|ISPY1_1002|    5|    T01|   LDIAM|Longest Diameter|     29|      mm|      29|      29|      mm|     MRI|       1|  MR1|1984-11-02|  2|\n",
      "|ISPY1_1001|    2|    T01|   LDIAM|Longest Diameter|     78|      mm|      78|      78|      mm|     MRI|       2|  MR2|1984-11-08|  3|\n",
      "+----------+-----+-------+--------+----------------+-------+--------+--------+--------+--------+--------+--------+-----+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check_cats.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "49bb3df9-05d6-49e0-8113-6fad1ad1b4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['USUBJID', 'TRSEQ', 'TRLNKID', 'TRTESTCD', 'TRTEST', 'TRORRES', 'TRORRESU', 'TRSTRESC', 'TRSTRESN', 'TRSTRESU', 'TRMETHOD', 'VISITNUM', 'VISIT', 'TRDTC', 'id']\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "79850e97-415d-4c0a-980a-ccc8bc08f5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.orderBy('USUBJID','VISITNUM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "feae5c86-d4ee-42ab-a3bf-6145641d4777",
   "metadata": {},
   "outputs": [],
   "source": [
    "LDIAM = df.filter(col(\"TRTESTCD\") == \"LDIAM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6a43384f-7132-4870-ba5d-0ade968a2b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-------+--------+----------------+-------+--------+--------+--------+--------+--------+--------+-----+----------+---+\n",
      "|   USUBJID|TRSEQ|TRLNKID|TRTESTCD|          TRTEST|TRORRES|TRORRESU|TRSTRESC|TRSTRESN|TRSTRESU|TRMETHOD|VISITNUM|VISIT|     TRDTC| id|\n",
      "+----------+-----+-------+--------+----------------+-------+--------+--------+--------+--------+--------+--------+-----+----------+---+\n",
      "|ISPY1_1001|    1|    T01|   LDIAM|Longest Diameter|     88|      mm|      88|      88|      mm|     MRI|       1|  MR1|1984-10-13|  1|\n",
      "|ISPY1_1001|    2|    T01|   LDIAM|Longest Diameter|     78|      mm|      78|      78|      mm|     MRI|       2|  MR2|1984-11-08|  3|\n",
      "|ISPY1_1001|    3|    T01|   LDIAM|Longest Diameter|     30|      mm|      30|      30|      mm|     MRI|       3|  MR3|1985-01-10| 10|\n",
      "|ISPY1_1001|    4|    T01|   LDIAM|Longest Diameter|     14|      mm|      14|      14|      mm|     MRI|       4|  MR4|1985-04-13| 27|\n",
      "|ISPY1_1002|    5|    T01|   LDIAM|Longest Diameter|     29|      mm|      29|      29|      mm|     MRI|       1|  MR1|1984-11-02|  2|\n",
      "|ISPY1_1002|    6|    T01|   LDIAM|Longest Diameter|     26|      mm|      26|      26|      mm|     MRI|       2|  MR2|1984-11-30|  5|\n",
      "|ISPY1_1002|    7|    T01|   LDIAM|Longest Diameter|     66|      mm|      66|      66|      mm|     MRI|       3|  MR3|1985-02-06| 13|\n",
      "|ISPY1_1002|    8|    T01|   LDIAM|Longest Diameter|     16|      mm|      16|      16|      mm|     MRI|       4|  MR4|1985-05-09| 34|\n",
      "|ISPY1_1003|    9|    T01|   LDIAM|Longest Diameter|     50|      mm|      50|      50|      mm|     MRI|       1|  MR1|1984-11-22|  4|\n",
      "|ISPY1_1003|   10|    T01|   LDIAM|Longest Diameter|     64|      mm|      64|      64|      mm|     MRI|       2|  MR2|1984-12-13|  7|\n",
      "|ISPY1_1003|   11|    T01|   LDIAM|Longest Diameter|     54|      mm|      54|      54|      mm|     MRI|       3|  MR3|1985-02-08| 14|\n",
      "|ISPY1_1003|   12|    T01|   LDIAM|Longest Diameter|     46|      mm|      46|      46|      mm|     MRI|       4|  MR4|1985-05-09| 35|\n",
      "|ISPY1_1004|   13|    T01|   LDIAM|Longest Diameter|     91|      mm|      91|      91|      mm|     MRI|       1|  MR1|1984-12-05|  6|\n",
      "|ISPY1_1004|   14|    T01|   LDIAM|Longest Diameter|     90|      mm|      90|      90|      mm|     MRI|       2|  MR2|1984-12-20|  8|\n",
      "|ISPY1_1004|   15|    T01|   LDIAM|Longest Diameter|     99|      mm|      99|      99|      mm|     MRI|       3|  MR3|1985-01-12| 12|\n",
      "|ISPY1_1004|   16|    T01|   LDIAM|Longest Diameter|     43|      mm|      43|      43|      mm|     MRI|       4|  MR4|1985-03-21| 21|\n",
      "|ISPY1_1005|   17|    T01|   LDIAM|Longest Diameter|     98|      mm|      98|      98|      mm|     MRI|       1|  MR1|1984-12-23|  9|\n",
      "|ISPY1_1005|   18|    T01|   LDIAM|Longest Diameter|    109|      mm|     109|     109|      mm|     MRI|       2|  MR2|1985-01-11| 11|\n",
      "|ISPY1_1005|   19|    T01|   LDIAM|Longest Diameter|     60|      mm|      60|      60|      mm|     MRI|       3|  MR3|1985-03-18| 20|\n",
      "|ISPY1_1005|   20|    T01|   LDIAM|Longest Diameter|     42|      mm|      42|      42|      mm|     MRI|       4|  MR4|1985-06-07| 46|\n",
      "+----------+-----+-------+--------+----------------+-------+--------+--------+--------+--------+--------+--------+-----+----------+---+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "LDIAM.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0ba073d4-882a-4156-ba56-f28d268f674c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[USUBJID: string, TRSEQ: string, TRLNKID: string, TRTESTCD: string, TRTEST: string, TRORRES: string, TRORRESU: string, TRSTRESC: string, TRSTRESN: float, TRSTRESU: string, TRMETHOD: string, VISITNUM: string, VISIT: string, TRDTC: string, id: int]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LDIAM.withColumn(\"TRSTRESN\",col(\"TRSTRESN\").cast(\"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7df1e298-d1ef-4082-9cb6-06659cc78703",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/11 00:05:04 ERROR Executor: Exception in task 0.0 in stage 71.0 (TID 53)\n",
      "org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value '61.7' of the type \"STRING\" cannot be cast to \"BIGINT\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\n",
      "== DataFrame ==\n",
      "\"__gt__\" was called from\n",
      "line 1 in cell [76]\n",
      "\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:145)\n",
      "\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.withException(UTF8StringUtils.scala:51)\n",
      "\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.toLongExact(UTF8StringUtils.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils.toLongExact(UTF8StringUtils.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat scala.collection.Iterator.isEmpty(Iterator.scala:466)\n",
      "\tat scala.collection.Iterator.isEmpty$(Iterator.scala:466)\n",
      "\tat scala.collection.AbstractIterator.isEmpty(Iterator.scala:1306)\n",
      "\tat scala.collection.IterableOnceOps.nonEmpty(IterableOnce.scala:967)\n",
      "\tat scala.collection.IterableOnceOps.nonEmpty$(IterableOnce.scala:967)\n",
      "\tat scala.collection.AbstractIterator.nonEmpty(Iterator.scala:1306)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1567)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2$adapted(RDD.scala:1566)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:918)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:918)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/06/11 00:05:04 WARN TaskSetManager: Lost task 0.0 in stage 71.0 (TID 53) (192.168.1.35 executor driver): org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value '61.7' of the type \"STRING\" cannot be cast to \"BIGINT\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\n",
      "== DataFrame ==\n",
      "\"__gt__\" was called from\n",
      "line 1 in cell [76]\n",
      "\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:145)\n",
      "\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.withException(UTF8StringUtils.scala:51)\n",
      "\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.toLongExact(UTF8StringUtils.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils.toLongExact(UTF8StringUtils.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat scala.collection.Iterator.isEmpty(Iterator.scala:466)\n",
      "\tat scala.collection.Iterator.isEmpty$(Iterator.scala:466)\n",
      "\tat scala.collection.AbstractIterator.isEmpty(Iterator.scala:1306)\n",
      "\tat scala.collection.IterableOnceOps.nonEmpty(IterableOnce.scala:967)\n",
      "\tat scala.collection.IterableOnceOps.nonEmpty$(IterableOnce.scala:967)\n",
      "\tat scala.collection.AbstractIterator.nonEmpty(Iterator.scala:1306)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1567)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2$adapted(RDD.scala:1566)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:918)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:918)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "25/06/11 00:05:04 ERROR TaskSetManager: Task 0 in stage 71.0 failed 1 times; aborting job\n",
      "{\"ts\": \"2025-06-11 00:05:04.078\", \"level\": \"ERROR\", \"logger\": \"DataFrameQueryContextLogger\", \"msg\": \"[CAST_INVALID_INPUT] The value '61.7' of the type \\\"STRING\\\" cannot be cast to \\\"BIGINT\\\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\", \"context\": {\"file\": \"line 1 in cell [76]\", \"line\": \"\", \"fragment\": \"__gt__\", \"errorClass\": \"CAST_INVALID_INPUT\"}, \"exception\": {\"class\": \"Py4JJavaError\", \"msg\": \"An error occurred while calling o1443.showString.\\n: org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value '61.7' of the type \\\"STRING\\\" cannot be cast to \\\"BIGINT\\\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\\n== DataFrame ==\\n\\\"__gt__\\\" was called from\\nline 1 in cell [76]\\n\\n\\tat org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:145)\\n\\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.withException(UTF8StringUtils.scala:51)\\n\\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.toLongExact(UTF8StringUtils.scala:31)\\n\\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils.toLongExact(UTF8StringUtils.scala)\\n\\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\\n\\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\\n\\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\\n\\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\\n\\tat scala.collection.Iterator.isEmpty(Iterator.scala:466)\\n\\tat scala.collection.Iterator.isEmpty$(Iterator.scala:466)\\n\\tat scala.collection.AbstractIterator.isEmpty(Iterator.scala:1306)\\n\\tat scala.collection.IterableOnceOps.nonEmpty(IterableOnce.scala:967)\\n\\tat scala.collection.IterableOnceOps.nonEmpty$(IterableOnce.scala:967)\\n\\tat scala.collection.AbstractIterator.nonEmpty(Iterator.scala:1306)\\n\\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1567)\\n\\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2$adapted(RDD.scala:1566)\\n\\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:918)\\n\\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:918)\\n\\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\\n\\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\\n\\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\\n\\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\\n\\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\\n\\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\\n\\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\\n\\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\\n\\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\\n\\tat java.base/java.lang.Thread.run(Thread.java:840)\\n\\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\\n\\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\\n\\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2579)\\n\\tat org.apache.spark.rdd.RDD.$anonfun$reduce$1(RDD.scala:1148)\\n\\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\\n\\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\\n\\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\\n\\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1130)\\n\\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$1(RDD.scala:1576)\\n\\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\\n\\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\\n\\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\\n\\tat org.apache.spark.rdd.RDD.takeOrdered(RDD.scala:1563)\\n\\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.$anonfun$executeCollect$1(limit.scala:327)\\n\\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:260)\\n\\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\\n\\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:257)\\n\\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:321)\\n\\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:402)\\n\\tat org.apache.spark.sql.execution.adaptive.ResultQueryStageExec.$anonfun$doMaterialize$1(QueryStageExec.scala:325)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$4(SQLExecution.scala:318)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$3(SQLExecution.scala:316)\\n\\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$2(SQLExecution.scala:312)\\n\\tat java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1768)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\\n\\tat java.base/java.lang.Thread.run(Thread.java:840)\\n\", \"stacktrace\": [{\"class\": null, \"method\": \"deco\", \"file\": \"/opt/anaconda3/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py\", \"line\": \"282\"}, {\"class\": null, \"method\": \"get_return_value\", \"file\": \"/opt/anaconda3/lib/python3.12/site-packages/py4j/protocol.py\", \"line\": \"327\"}]}}\n"
     ]
    },
    {
     "ename": "NumberFormatException",
     "evalue": "[CAST_INVALID_INPUT] The value '61.7' of the type \"STRING\" cannot be cast to \"BIGINT\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\n== DataFrame ==\n\"__gt__\" was called from\nline 1 in cell [76]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNumberFormatException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m LDIAM\u001b[38;5;241m.\u001b[39mfilter((col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVISIT\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMR1\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m&\u001b[39m (col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTRSTRESN\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m88\u001b[39m))\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pyspark/sql/classic/dataframe.py:285\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_show_string(n, truncate, vertical))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pyspark/sql/classic/dataframe.py:303\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    298\u001b[0m         errorClass\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    299\u001b[0m         messageParameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    300\u001b[0m     )\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mshowString(n, \u001b[38;5;241m20\u001b[39m, vertical)\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/py4j/java_gateway.py:1362\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1363\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:288\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    284\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mNumberFormatException\u001b[0m: [CAST_INVALID_INPUT] The value '61.7' of the type \"STRING\" cannot be cast to \"BIGINT\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\n== DataFrame ==\n\"__gt__\" was called from\nline 1 in cell [76]\n"
     ]
    }
   ],
   "source": [
    "LDIAM.filter((col(\"VISIT\") == \"MR1\") & (col(\"TRSTRESN\") > 88)).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
