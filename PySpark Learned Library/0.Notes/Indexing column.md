

# Indexing in PySpark ‚Äî Full Summary

PySpark **does not have a built-in ‚Äúindex‚Äù** like pandas.
But you can still create **row indices**, and you can access **columns by index position** programmatically.

Let‚Äôs break both down clearly üëá

---

## **1Ô∏è‚É£ Row Indexing (Creating Row Numbers or IDs)**

Since PySpark DataFrames are **distributed and unordered**, there‚Äôs no default index.
If you need one, here are the main methods:

---

### **A. `monotonically_increasing_id()`**

Adds a **unique numeric ID** column to each row.

```python
from pyspark.sql.functions import monotonically_increasing_id

df_indexed = df.withColumn("index", monotonically_increasing_id())
```

‚úÖ Each row gets a unique 64-bit integer.
‚ö†Ô∏è Not sequential (due to partitions).
Example output:

| Name | Dept | index      |
| ---- | ---- | ---------- |
| John | IT   | 0          |
| Amy  | HR   | 1          |
| Max  | IT   | 8589934592 |

---

### **B. `zipWithIndex()` (Sequential Index)**

For **strictly sequential indices (0, 1, 2, 3 ‚Ä¶)**:

```python
df_indexed = df.rdd.zipWithIndex().map(lambda x: (*x[0], x[1])).toDF(df.columns + ["index"])
```

‚úÖ Produces sequential order
‚ö†Ô∏è Slower because it converts between RDD and DataFrame.

---

### **C. `row_number()` Window Function**

For **ordered** sequential numbering based on a column:

```python
from pyspark.sql.window import Window
from pyspark.sql.functions import row_number

windowSpec = Window.orderBy("Salary")
df_indexed = df.withColumn("index", row_number().over(windowSpec))
```

‚úÖ Sequential and deterministic
‚úÖ You can order by specific columns
‚ö†Ô∏è More expensive for large data due to sorting.

---

## üßÆ **2Ô∏è‚É£ Column Indexing (Selecting Columns by Position)**

In PySpark, you can access columns by **name** or by **index position** using `df.columns`.

---

### **A. Select Columns by Index Range**

```python
X = df.select([df.columns[i] for i in range(0, 7)])
```

‚úÖ Selects columns **0 through 6** (like slicing).
‚úÖ Useful when you don‚Äôt want to type column names manually.

---

### **B. Select a Single Column by Index**

```python
col8 = df.select([df.columns[8]])
```

If you want it as a **Python list** (e.g., for labels `y`):

```python
y = [x[0] for x in df.select([df.columns[i] for i in [8]]).collect()]
```

üîç Breakdown:

* `df.columns[i]` ‚Üí gets column name by position (e.g., `"Salary"`).
* `df.select([...])` ‚Üí selects it as a DataFrame.
* `.collect()` ‚Üí brings all rows to driver (be careful with large data).
* `[x[0] for x in ...]` ‚Üí extracts the column values into a flat Python list.

‚úÖ Works well for small datasets or ML splits (X, y).
‚ö†Ô∏è Not scalable for big DataFrames (use `.rdd` for large ones).

---

## üß† **3Ô∏è‚É£ Accessing Data by Index or Value**

* **Row indexing** is manual (you must add your own index column).
* **Column indexing** can be done via `df.columns[i]`.
* There‚Äôs no `.iloc` or `.loc` like in pandas ‚Äî you use `.filter()` or `.where()` instead.

Example:

```python
df_indexed.filter(df_indexed.index == 5).show()
```

---

## üßæ **4Ô∏è‚É£ Summary Table**

| Goal                          | Method                                               | Example                                              | Notes                         |
| ----------------------------- | ---------------------------------------------------- | ---------------------------------------------------- | ----------------------------- |
| Add unique ID                 | `monotonically_increasing_id()`                      | `df.withColumn("id", monotonically_increasing_id())` | Fast, not sequential          |
| Add sequential index          | `zipWithIndex()`                                     | `df.rdd.zipWithIndex()`                              | Exact sequence, slower        |
| Ordered numbering             | `row_number()`                                       | `row_number().over(Window.orderBy("col"))`           | Deterministic, requires order |
| Select columns by index range | `df.select([df.columns[i] for i in range(a,b)])`     | Flexible for subsets                                 |                               |
| Select column as list         | `[x[0] for x in df.select(df.columns[i]).collect()]` | Good for small data                                  |                               |
| Access rows by index          | `df.filter(df.index == n)`                           | Manual filtering                                     |                               |

---

## ‚öôÔ∏è **5Ô∏è‚É£ Key Takeaways**

‚úÖ PySpark has **no built-in index** ‚Äî you must create one manually.
‚úÖ For **small datasets**, `.collect()` + list comprehensions are fine.
‚úÖ For **large datasets**, avoid `.collect()` ‚Äî use RDD operations or keep data distributed.
‚úÖ Use **window functions** for ordered, sequential numbering when order matters.

