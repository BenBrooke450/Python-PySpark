
# **`DataFrame.orderBy()` — Summary**

`orderBy()` is the **DataFrame API method for sorting rows** in PySpark, similar to SQL’s `ORDER BY`. It produces a **new DataFrame** with rows globally sorted by the specified columns.

---

# **1. Basic Usage**

```python
df.orderBy("col1")  # sort ascending by default
df.orderBy(df["col1"].desc())  # descending
df.orderBy("col1", "col2")  # sort by multiple columns
```

**Notes:**

* Returns a **new DataFrame** — does not modify the original.
* Sorting is **stable** within Spark’s distributed execution model.
* You can mix ascending and descending columns.

---

# **2. Method Signature**

```python
DataFrame.orderBy(*cols, **kwargs)
```

**Parameters:**

| Parameter     | Description                                                                                  |
| ------------- | -------------------------------------------------------------------------------------------- |
| `*cols`       | Column names or Column expressions to sort by (string or Column object)                      |
| `ascending`   | Optional boolean or list of booleans (default True). If True → ascending, False → descending |
| `na_position` | Optional: `"first"` or `"last"` (where NULLs go; default `"last"`)                           |

---

# **3. Ascending / Descending Sorting**

### Single Column

```python
df.orderBy("age", ascending=False)
```

### Multiple Columns

```python
df.orderBy(["age", "salary"], ascending=[True, False])
```

* First sorts by `age` ascending
* Then sorts by `salary` descending if there are ties

---

# **4. Handling Null Values**

* Default: NULLs go **last**
* You can override:

```python
df.orderBy(df["age"].asc_nulls_first())  # NULLs first
df.orderBy(df["age"].desc_nulls_last())  # NULLs last
```

---

# **5. How `orderBy()` Works Internally**

### 5.1 Distributed Sort

* Spark DataFrames are partitioned
* Sorting is **global**, so Spark must **shuffle rows between partitions**
* Uses **range partitioning** and **sorting inside partitions**:

  1. Compute a **range of keys** using sampling
  2. Redistribute rows to partitions based on key range (shuffle)
  3. Sort rows **within each partition**
* Result: fully globally sorted DataFrame

---

### 5.2 Partitioning After Sorting

* Default: Number of output partitions = number of input partitions
* You can repartition manually if needed for better performance

```python
df.orderBy("age").coalesce(5)  # reduce to 5 partitions after sort
```

---

# **6. Performance Considerations**

* Sorting triggers a **full shuffle** → expensive
* Cost increases with:

  * Number of partitions
  * Size of data
  * Number of sort columns
* Avoid sorting large datasets unless necessary
* If you need **only top N rows**, use `df.orderBy("col").limit(N)` → avoids full shuffle if Spark can optimize

---

# **7. Differences from RDD `sortBy()`**

| Feature             | DataFrame `orderBy()`          | RDD `sortBy()`            |
| ------------------- | ------------------------------ | ------------------------- |
| Input               | Column names / expressions     | Python function (lambda)  |
| Optimization        | Catalyst + Tungsten            | Pure Python / JVM, slower |
| Shuffle             | Required for global sort       | Required                  |
| Performance         | Optimized for distributed sort | Slower                    |
| Null handling       | Built-in (`asc_nulls_first`)   | Needs manual handling     |
| Global vs Partition | Global sort                    | Global sort via shuffle   |

---

# **8. Example Usages**

### 8.1 Sort ascending by a single column

```python
df.orderBy("age").show()
```

### 8.2 Sort descending

```python
df.orderBy(df["salary"].desc()).show()
```

### 8.3 Sort multiple columns

```python
df.orderBy(df["age"].asc(), df["salary"].desc()).show()
```

### 8.4 Handling nulls

```python
df.orderBy(df["age"].asc_nulls_first()).show()
```

### 8.5 Using string column names

```python
df.orderBy("age", "salary").show()
```

---

# **9. orderBy vs sort**

* `df.sort()` is **an alias** for `df.orderBy()`
* Both work identically

```python
df.sort("age") == df.orderBy("age")
```

---

# **10. Tips and Best Practices**

1. **Minimize shuffles**:

   * Avoid sorting unnecessarily in pipelines
   * Use `limit()` when only top N rows are needed

2. **Use Column expressions for clarity**:

   ```python
   from pyspark.sql.functions import col
   df.orderBy(col("age").desc())
   ```

3. **For deterministic results**:

   * Always include a secondary column if primary column has duplicates
   * Example: `df.orderBy("age", "name")`

4. **Avoid mixing RDD and DataFrame sorting** unless necessary:

   * DataFrame API is **more efficient**
   * RDD API requires Python lambda functions and cannot leverage Catalyst

---

# **11. Summary (Short Version)**

* `DataFrame.orderBy()` sorts rows **globally** across all partitions
* Takes **columns or column expressions** for sorting
* Supports **ascending / descending** order and **NULL handling**
* Returns a **new DataFrame**
* Triggers a **full shuffle**, so it is expensive on large datasets
* `df.sort()` is just an alias
* Use only when you need a **deterministically ordered DataFrame**

